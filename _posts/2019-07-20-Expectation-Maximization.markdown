---
layout: post
title:  "An Introduction to Expectation Maximization with Binomial Distributions"
date:   2019-07-20 21:00:00 -0400
categories: design
published: false
---
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
<!-- Youâ€™ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets: -->
# Introduction
Consider a pair of unfair coins, with some probabilities $$p_1,p_2$$ of heads,
and $$1-p_1,1-p_2$$ probabilities of tails.
These coins are tossed, unlabeled, and the results
of the tosses are recorded. A process to identify
parameters of the underlying distribution, or identify
the probability of heads for each coin,
and the coin that was chosen (latent variable) is Expectation Maximization.

Expectation Maximization is the process of computing
the expected value of the log-likelihood function
with respect to the conditional distribution of assignments
given observations and parameters of the individual distributions,
and then using the expected value to maximize
the likelihood of the observed data with respect
to the chosen model by tuning the parameters of the model.

## Maximum Likelihood Estimation

A simple, intuitive example of this is 
identifying the bias of an unfair coin.

<script src="https://gist.github.com/cmorterud/084e3298581fc1e91128916bd5a9af03.js"></script>

![image](/assets/images/20_choose_15_likelihood_graph.png "A graph showing the likelihood of 20 choose 15 with respect to a binomial model")

The graph shows the likelihood of $$f(p)={20\choose15}(p)^{15}(1-p)^5$$ for $$[0.0, 1.0]$$
maximizing at 0.75. Thus, 0.75 maximizes the likelihood of the observations.
This is an intuitive result, yet demonstrates the idea of likelihood maximization.

A more direct approach to identifying the parameter that maximizes
the likelihood function is through differentiation.


$$
\begin{align}
f(p)=&{20\choose15}(p)^{15}(1-p)^5\\
\frac{d}{dp}{20\choose15}(p)^{15}(1-p)^5=&15p^{14}(1-p)^5-5(1-p)^4p^{15}\\
0=&p^{14}(1-p)^4(15(1-p)-5p)\\
0=&p^{14}(1-p)^4(15-15p-5p)\\
0=&p^{14}(1-p)^4(15-20p)
\end{align}
$$


The solutions are 0, 1, and 0.75. A probability of 0 or 1 is non-sensical
for a coin with a non-zero probability, so 0.75 represents
where the likelihood is maximized.

# Algorithm
## Definitions
Consider $$X$$, the set of observed data, where each data point
is the number of heads generated by a single coin in an experiment.
For example, an experiment with 4 flips, and a sequence of HTHH has 3 heads
$$x^{(i)}$$ is the number of heads for the $$i$$th trial.
flipped repeatedly. Consider $$Z$$, the assignments
to coin 1 or coin 2 for $$x\in X$$, which is the latent,
or missing data. For example, the assignment could be "coin 1" for a datapoint.
$$z^{(i)}$$ is the coin used for the $$i$$th trial.
Consider $$\theta$$, the parameter for each coin, namely
the probability of a heads upon flip, which is a number on $$[0,1]$$.
$$\theta_k$$ is the probability of heads associated with the $$k$$th coin.
Let there be $$N$$ observations of heads or tails per trial.

Let's initialize the prior probability of a given observation to be assigned
to a particular coin to be $$\frac{1}{2}$$, an equal chance of being
labeled as "coin 1" or "coin 2". Namely $$z^{(i)}=\frac{1}{2}$$,
because there are two coins.

$$p(x^{(i)}|z^{(i)}=k;\theta)={N\choose x^{(i)}}\theta_k^{x^{(i)}}(1-\theta_k)^{N-x^{(i)}}$$

## Expectation Step
For the expectation step, given the parameters and observations,
namely $$\theta$$ and $$X$$, calculate the probability
that given series of coin flips, or observation, used
coin $$i$$. That is, the probability of the latent variable.

$$
\begin{align}
p(z^{(i)}=k|x^{(i)};\theta)=&\frac{p(z^{(i)}=k,x^{(i)};\theta)}{p(x^{(i)};\theta)}\\
=&\frac{p(z^{(i)}=k)p(x^{(i)}|z^{(i)}=k;\theta)}{p(x^{(i)};\theta)}\\
=&\frac{\frac{1}{2}{N\choose x^{(i)}}\theta_k^{x^{(i)}}(1-\theta_k)^{N-x^{(i)}}}{p(x^{(i)};\theta)}\\
=&\frac{\frac{1}{2}{N\choose x^{(i)}}\theta_k^{x^{(i)}}(1-\theta_k)^{N-x^{(i)}}}{\sum_{z\in\{1, 2\}}\frac{1}{2}{N\choose x^{(i)}}\theta_z^{x^{(i)}}(1-\theta_z)^{N-x^{(i)}}}
\end{align}
$$


## Maximization Step

# Sources
[N choose K code](https://stackoverflow.com/questions/4941753/is-there-a-math-ncr-function-in-python)

# Contact
Please feel free to email me at
[{{ site.email }}](mailto:{{ site.email }})
with any questions or concerns. Thanks!